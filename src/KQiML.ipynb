{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafe5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from PIL import Image\n",
    "import pennylane as qml\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92817404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckUnitary(U, tol=1e-5):\n",
    "    I = torch.eye(U.shape[0], dtype=U.dtype, device=U.device)\n",
    "    UU_dagger = U @ U.conj().T\n",
    "    deviation = torch.norm(UU_dagger - I)\n",
    "    #print(f\"Deviation from unitarity: {deviation.item()}\")\n",
    "    return deviation < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e72ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # Represent a unitary via a parameterized Hermitian matrix (U = exp(-iH))\n",
    "        self.H = nn.Parameter(torch.randn(input_dim, input_dim, dtype=torch.complex128))\n",
    "\n",
    "    def forward(self, psi):\n",
    "        # Create unitary with U = exp(-iH) using matrix exponential\n",
    "        H_hermitian = 0.5 * (self.H + self.H.conj().T)\n",
    "        U = torch.matrix_exp(-1j * H_hermitian)\n",
    "\n",
    "        if CheckUnitary(U):\n",
    "            out_state = U @ psi.T\n",
    "            #print(\"Unitary check passed ✅\")\n",
    "        else:\n",
    "            out_state = U @ psi.T\n",
    "            print(\"Unitary check failed ❌\")\n",
    "        \n",
    "        return out_state.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AncillaLayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # Controlled or entangling block\n",
    "        self.H = nn.Parameter(torch.randn(2*dim, 2*dim, dtype=torch.complex128))  # Ancilla + psi\n",
    "\n",
    "    def forward(self, psi):\n",
    "        batch_size = psi.shape[0]\n",
    "        dim = psi.shape[1]\n",
    "\n",
    "        ancilla = torch.zeros(batch_size, 2, dtype=torch.complex128, device=psi.device)\n",
    "        ancilla[:, 0] = 1.0  # |0⟩\n",
    "\n",
    "        joint_state = (ancilla.unsqueeze(2) * psi.unsqueeze(1)).reshape(batch_size, 2 * dim)\n",
    "\n",
    "        H_hermitian = 0.5 * (self.H + self.H.conj().T)\n",
    "        U = torch.matrix_exp(-1j * H_hermitian)\n",
    "        out_state = U @ joint_state.T\n",
    "        return out_state.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1721dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Measurement(nn.Module):\n",
    "    def forward(self, psi, num_ancilla):\n",
    "        vector_length = psi.shape[1]\n",
    "        # Extract ancilla = 0 portion\n",
    "        psi_sys0 = psi[:, :vector_length//(2**num_ancilla)]  # assuming ancilla is the first qubit\n",
    "        # Normalize (since you're post-selecting, it's not a full probability distribution)\n",
    "        psi_sys0 = psi_sys0 / psi_sys0.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        '''\n",
    "        # Extract ancilla = 1 portion\n",
    "        psi_sys1 = psi[:, vector_length//2:]  # assuming ancilla is the first qubit\n",
    "        # Normalize\n",
    "        psi_sys1 = psi_sys1 / psi_sys1.norm(dim=1, keepdim=True)\n",
    "        '''\n",
    "\n",
    "        # Measurement:\n",
    "        probs = torch.abs(psi_sys0) ** 2\n",
    "        return psi_sys0, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a92c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QiNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers, num_ancilla):\n",
    "        super().__init__()\n",
    "        self.num_ancilla = num_ancilla\n",
    "        self.quantum_layers_with_ancilla = nn.ModuleList([QuantumLayer((2**num_ancilla)*input_dim) for _ in range(num_layers)])\n",
    "        self.measure = Measurement()\n",
    "\n",
    "    def forward(self, psi):\n",
    "        \n",
    "        '''\n",
    "        joint_state = psi.clone()\n",
    "\n",
    "        for i in range(self.num_ancilla):\n",
    "            batch_size = joint_state.shape[0]\n",
    "            dim = joint_state.shape[1]\n",
    "            ancilla = torch.zeros(batch_size, 2, dtype=torch.complex128, device=psi.device)\n",
    "            ancilla[:, 0] = 1.0  # |0>\n",
    "            joint_state = (ancilla.unsqueeze(2) * joint_state.unsqueeze(1)).reshape(batch_size, 2 * dim)\n",
    "\n",
    "            joint_state = joint_state / joint_state.norm(dim=1, keepdim=True)\n",
    "        '''\n",
    "\n",
    "        batch_size = psi.shape[0]\n",
    "        dim = psi.shape[1]\n",
    "\n",
    "        joint_state = torch.tensor(np.zeros((batch_size,(2**self.num_ancilla)*dim), dtype=complex), dtype=torch.complex128)\n",
    " \n",
    "\n",
    "        ancilla_1 = torch.tensor([1, 0], dtype=torch.complex128)\n",
    "        ancilla = torch.tensor([1], dtype=torch.complex128)\n",
    "\n",
    "        for j in range(self.num_ancilla):     \n",
    "            ancilla =  torch.kron(ancilla_1, ancilla)\n",
    "\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            #print(ancilla)\n",
    "            joint_state[j,:] = torch.kron(ancilla, psi[j,:])\n",
    "\n",
    "\n",
    "        joint_state = joint_state / joint_state.norm(dim=1, keepdim=True)\n",
    "        \n",
    "        for layer in self.quantum_layers_with_ancilla:\n",
    "            joint_state = layer(joint_state)\n",
    "            \n",
    "        psi_sys0, probs = self.measure(joint_state, self.num_ancilla)\n",
    "        return psi_sys0, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d27338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(N, new_size, number_train_samples, data_dir):\n",
    "    def load_MNIST_dataset_Images(file_path): # for now trial on mnist dataset\n",
    "        images = idx2numpy.convert_from_file(file_path)\n",
    "        images = images.reshape(images.shape[0], -1)\n",
    "        images = images.astype(np.float64)\n",
    "        return images\n",
    "\n",
    "    def load_MNIST_dataset_Labels(file_path):\n",
    "        labels = idx2numpy.convert_from_file(file_path)\n",
    "        return labels\n",
    "\n",
    "    # Construct full paths\n",
    "    train_images_file = os.path.join(data_dir, \"train-images.idx3-ubyte\")\n",
    "    train_labels_file = os.path.join(data_dir, \"train-labels.idx1-ubyte\")\n",
    "\n",
    "\n",
    "    mnist_images = load_MNIST_dataset_Images(train_images_file)\n",
    "    mnist_labels = load_MNIST_dataset_Labels(train_labels_file)\n",
    "\n",
    "    # Separating the load dataset into train labels and the train digits \n",
    "\n",
    "    def separate_images_and_labels(images , labels , digit):\n",
    "        digit_images = []\n",
    "        digit_labels = []\n",
    "        for image,label in zip(images,labels):\n",
    "            if label == digit:\n",
    "                digit_images.append(image)\n",
    "                digit_labels.append(label)\n",
    "        return digit_images,digit_labels\n",
    "\n",
    "    digits_to_separate = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    digit_image = {}\n",
    "    digit_label = {}\n",
    "\n",
    "    for digit in digits_to_separate:\n",
    "        digit_image[digit], digit_label[digit] = separate_images_and_labels(mnist_images,mnist_labels,digit)\n",
    "\n",
    "    num_samples = 10 \n",
    "    '''\n",
    "    for digit in digit_image.keys():\n",
    "        sample_images = digit_image[digit][:num_samples]\n",
    "        plt.figure(figsize = (10,2))\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(1, num_samples, i + 1)\n",
    "            plt.imshow(sample_images[i].reshape(28,28), cmap = 'gray')\n",
    "            plt.title(f\"Digit: {digit}\")\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "    '''\n",
    "    # Resize the image to a higher accuracy and match the dimensions of the qubits\n",
    "\n",
    "    from skimage.transform import resize\n",
    "\n",
    "    def resize_images(images, new_size = (32,32)):\n",
    "        resized_images = []\n",
    "        for image in images:\n",
    "            resized_image = resize(image.reshape(28,28), new_size)\n",
    "            resized_images.append(resized_image.flatten())\n",
    "        return resized_images\n",
    "\n",
    "    resized_digit_images = {}\n",
    "    for digit in digit_image.keys():\n",
    "        resized_digit_images[digit] = resize_images(digit_image[digit], new_size = new_size)\n",
    "\n",
    "    '''\n",
    "    for digit in digit_image.keys():\n",
    "        sample_images = resized_digit_images[digit][:num_samples]\n",
    "        plt.figure(figsize =(10,2))\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(1, num_samples, i+1)\n",
    "            plt.imshow(sample_images[i].reshape(new_size), cmap = 'magma')\n",
    "            plt.title(f\"Digit:{digit}\")\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "    '''\n",
    "\n",
    "    # Converting all the digits in to arrays\n",
    "    Digit_zero = []\n",
    "\n",
    "    for image in resized_digit_images[0]:\n",
    "        Digit_zero_ = image\n",
    "        Digit_zero.append(Digit_zero_)\n",
    "\n",
    "    Digit_zero = np.array(Digit_zero)\n",
    "\n",
    "    #print('Digit zero',Digit_zero)\n",
    "\n",
    "    Digit_one = []\n",
    "\n",
    "    for image in resized_digit_images[1]:\n",
    "        Digit_one_ = image\n",
    "        Digit_one.append(Digit_one_)\n",
    "\n",
    "    Digit_one = np.array(Digit_one)\n",
    "\n",
    "    #print('Digit one',Digit_one)\n",
    "\n",
    "    Digit_two = []\n",
    "\n",
    "    for image in resized_digit_images[2]:\n",
    "        Digit_two_ = image\n",
    "        Digit_two.append(Digit_two_)\n",
    "\n",
    "    Digit_two = np.array(Digit_two)\n",
    "\n",
    "    #print('Digit two',Digit_two)\n",
    "\n",
    "    Digit_three = []\n",
    "\n",
    "    for image in resized_digit_images[3]:\n",
    "        Digit_three_ = image\n",
    "        Digit_three.append(Digit_three_)\n",
    "\n",
    "    Digit_three = np.array(Digit_three)\n",
    "\n",
    "    #print('Digit three',Digit_three)\n",
    "\n",
    "    Digit_four = []\n",
    "\n",
    "    for image in resized_digit_images[4]:\n",
    "        Digit_four_ = image\n",
    "        Digit_four.append(Digit_four_)\n",
    "\n",
    "    Digit_four = np.array(Digit_four)\n",
    "\n",
    "    #print('Digit four',Digit_four)\n",
    "\n",
    "    Digit_fifth = []\n",
    "\n",
    "    for image in resized_digit_images[5]:\n",
    "        Digit_fifth_ = image\n",
    "        Digit_fifth.append(Digit_fifth_)\n",
    "\n",
    "    Digit_fifth = np.array(Digit_fifth)\n",
    "\n",
    "    #print('Digit fifth',Digit_fifth)\n",
    "\n",
    "    Digit_sixth = []\n",
    "\n",
    "    for image in resized_digit_images[6]:\n",
    "        Digit_sixth_ = image\n",
    "        Digit_sixth.append(Digit_sixth_)\n",
    "\n",
    "    Digit_sixth = np.array(Digit_sixth)\n",
    "\n",
    "    #print('Digit Sixth',Digit_sixth)\n",
    "\n",
    "    Digit_seventh = []\n",
    "\n",
    "    for image in resized_digit_images[7]:\n",
    "        Digit_seventh_ = image\n",
    "        Digit_seventh.append(Digit_seventh_)\n",
    "\n",
    "    Digit_seventh = np.array(Digit_seventh)\n",
    "\n",
    "    #print('Digit seventh',Digit_seventh)\n",
    "\n",
    "    Digit_eigth = []\n",
    "\n",
    "    for image in resized_digit_images[8]:\n",
    "        Digit_eigth_ = image\n",
    "        Digit_eigth.append(Digit_eigth_)\n",
    "\n",
    "    Digit_eigth = np.array(Digit_eigth)\n",
    "\n",
    "    #print('Digit eigth',Digit_eigth)\n",
    "\n",
    "    Digit_nineth = []\n",
    "\n",
    "    for image in resized_digit_images[9]:\n",
    "        Digit_nineth_ = image\n",
    "        Digit_nineth.append(Digit_nineth_)\n",
    "\n",
    "    Digit_nineth = np.array(Digit_nineth)\n",
    "\n",
    "    #print('Digit nineth',Digit_nineth)\n",
    "\n",
    "    # Digit zero to Nine grey scale matrix conversion by reshaping a row array into a (N,N) size\n",
    "\n",
    "    # Quantum Amplitude Encoding of MNIST Dataset\n",
    "\n",
    "    Digit_zero_ = []\n",
    "\n",
    "    for i in range(len(Digit_zero)):\n",
    "        Digit_zero_.append((Digit_zero[i].reshape(N**2,1))/np.linalg.norm(Digit_zero[i]))\n",
    "\n",
    "    Digit_one_ = []\n",
    "\n",
    "    for i in range(len(Digit_one)):\n",
    "        Digit_one_.append((Digit_one[i].reshape(N**2,1))/np.linalg.norm(Digit_one[i]))\n",
    "\n",
    "    Digit_two_ = []\n",
    "\n",
    "    for i in range(len(Digit_two)):\n",
    "        Digit_two_.append((Digit_two[i].reshape(N**2,1))/np.linalg.norm(Digit_two[i]))\n",
    "\n",
    "    Digit_three_ = []\n",
    "\n",
    "    for i in range(len(Digit_three)):\n",
    "        Digit_three_.append((Digit_three[i].reshape(N**2,1))/np.linalg.norm(Digit_three[i]))\n",
    "\n",
    "    Digit_four_ = []\n",
    "\n",
    "    for i in range(len(Digit_four)):\n",
    "        Digit_four_.append((Digit_four[i].reshape(N**2,1))/np.linalg.norm(Digit_four[i]))\n",
    "\n",
    "    Digit_five_ = []\n",
    "\n",
    "    for i in range(len(Digit_fifth)):\n",
    "        Digit_five_.append((Digit_fifth[i].reshape(N**2,1))/np.linalg.norm(Digit_fifth[i]))\n",
    "\n",
    "    Digit_sixth_ = []\n",
    "\n",
    "    for i in range(len(Digit_sixth)):\n",
    "        Digit_sixth_.append((Digit_sixth[i].reshape(N**2,1))/np.linalg.norm(Digit_sixth[i]))\n",
    "\n",
    "    Digit_seventh_ = []\n",
    "\n",
    "    for i in range(len(Digit_seventh)):\n",
    "        Digit_seventh_.append((Digit_seventh[i].reshape(N**2,1))/np.linalg.norm(Digit_seventh[i]))\n",
    "\n",
    "    Digit_eigth_ = []\n",
    "\n",
    "    for i in range(len(Digit_eigth)):\n",
    "        Digit_eigth_.append((Digit_eigth[i].reshape(N**2,1))/np.linalg.norm(Digit_eigth[i]))\n",
    "\n",
    "    Digit_nineth_ = []\n",
    "\n",
    "    for i in range(len(Digit_nineth)):\n",
    "        Digit_nineth_.append((Digit_nineth[i].reshape(N**2,1))/np.linalg.norm(Digit_nineth[i]))\n",
    "\n",
    "\n",
    "    Input_state_vector_zero = np.array(Digit_zero_[:number_train_samples])\n",
    "    Input_state_vector_one = np.array(Digit_one_[:number_train_samples])\n",
    "    Input_state_vector_two = np.array(Digit_two_[:number_train_samples])\n",
    "    Input_state_vector_three = np.array(Digit_three_[:number_train_samples])\n",
    "    Input_state_vector_four = np.array(Digit_four_[:number_train_samples])\n",
    "    Input_state_vector_fifth = np.array(Digit_five_[:number_train_samples])\n",
    "    Input_state_vector_sixth = np.array(Digit_sixth_[:number_train_samples])\n",
    "    Input_state_vector_seventh = np.array(Digit_seventh_[:number_train_samples])\n",
    "    Input_state_vector_eigth = np.array(Digit_eigth_[:number_train_samples])\n",
    "    Input_state_vector_nineth = np.array(Digit_nineth_[:number_train_samples])\n",
    "\n",
    "    input_state_vectors = []\n",
    "\n",
    "    input_state_vectors.append(Input_state_vector_zero)\n",
    "    input_state_vectors.append(Input_state_vector_one)\n",
    "    input_state_vectors.append(Input_state_vector_two)\n",
    "    input_state_vectors.append(Input_state_vector_three)\n",
    "    input_state_vectors.append(Input_state_vector_four)\n",
    "    input_state_vectors.append(Input_state_vector_fifth)\n",
    "    input_state_vectors.append(Input_state_vector_sixth)\n",
    "    input_state_vectors.append(Input_state_vector_seventh)\n",
    "    input_state_vectors.append(Input_state_vector_eigth)\n",
    "    input_state_vectors.append(Input_state_vector_nineth)\n",
    "\n",
    "    input_state_vectors = np.array(input_state_vectors, dtype=complex)\n",
    "\n",
    "    input_state_vectors = input_state_vectors.reshape(number_train_samples*10,N**2)\n",
    "\n",
    "    input_state_vectors = np.array(input_state_vectors, dtype=complex)\n",
    "\n",
    "    # Convert numpy array to PyTorch tensor\n",
    "    input_vectors = torch.tensor(input_state_vectors, dtype=torch.complex128)\n",
    "\n",
    "    target_states_combined = np.zeros((number_train_samples*10,N**2), dtype=complex)\n",
    "\n",
    "    encode_length = (N**2)//9\n",
    "\n",
    "    for i in range(10):\n",
    "        start_row, end_row = i*number_train_samples, (i+1)*number_train_samples\n",
    "        start_col = i*encode_length\n",
    "\n",
    "        target_states_combined[start_row:end_row, start_col] = 1\n",
    "\n",
    "    target_states_combined = torch.tensor(target_states_combined, dtype=torch.complex128)\n",
    "\n",
    "    # Load MNIST test dataset\n",
    "\n",
    "    # Construct full paths\n",
    "    test_images_file = os.path.join(data_dir, \"t10k-images.idx3-ubyte\")\n",
    "    test_labels_file = os.path.join(data_dir, \"t10k-labels.idx1-ubyte\")\n",
    "\n",
    "\n",
    "    images = idx2numpy.convert_from_file(test_images_file)# Download the MNIST Dataset from Kaggle and Change this directory to your Directory\n",
    "    labelss = idx2numpy.convert_from_file(test_labels_file)\n",
    "\n",
    "    # Resize images to 32x32\n",
    "    images_resized = np.array([np.array(Image.fromarray(img).resize(new_size)) for img in images])\n",
    "\n",
    "    # Normalize vectors\n",
    "    test_images = images_resized \n",
    "\n",
    "    input_state_test_vectors = []\n",
    "\n",
    "\n",
    "    # Digit zero to Nine grey scale matrix conversion by reshaping a row array into a (N,N) size\n",
    "\n",
    "    for i in range(len(images_resized)):\n",
    "        input_state_test_vectors.append((images_resized[i].reshape(N**2,1))/np.linalg.norm(images_resized[i]))\n",
    "\n",
    "    input_state_test_vectors = np.array(input_state_test_vectors,dtype = np.complex128)\n",
    "    input_state_test_vectors = torch.tensor(input_state_test_vectors,dtype = torch.complex128)\n",
    "    input_state_test_vectors = input_state_test_vectors.squeeze(2)\n",
    "   \n",
    "\n",
    "    return input_vectors, target_states_combined, input_state_test_vectors, labelss, encode_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef025efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_layers_arr = [4, 8, 12, 24]\n",
    "N_arr = [4, 8, 16]\n",
    "number_train_samples_arr = [1000, 2000, 3000] #number of train images per class\n",
    "number_ancilla_arr = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ff5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   Pixels    Train Samples  Layers    Ancilla Qubits    Train Acc   Test Acc  Timestamp           \n",
      "1           16        1000           4         2                 90.0200     89.4000   2025-07-06 23:13:04\n",
      "2           16        1000           8         2                 88.1600     87.2600   2025-07-07 01:40:00\n",
      "3           16        1000           12        2                 89.1900     88.4300   2025-07-07 05:20:27\n",
      "4           16        1000           24        2                 63.6800     61.7900   2025-07-07 14:28:20\n",
      "5           16        2000           4         2                 86.0350     86.6700   2025-07-07 16:21:55\n",
      "6           16        2000           8         2                 88.3900     88.7300   2025-07-07 19:56:02\n",
      "7           16        2000           12        2                 84.2700     84.8900   2025-07-08 00:53:34\n"
     ]
    }
   ],
   "source": [
    "# Create a new directory with current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"results_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define output file path\n",
    "output_file = os.path.join(output_dir, \"results.csv\")\n",
    "\n",
    "# Write the header directly since it's a new file\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Iteration\", \"No: Pixels\", \"No: Train samples\", \"No: Layers\", \"No: Ancilla Qubits\", \"Train Accuracy\", \"Test Accuracy\", \"Time Stamp\"])\n",
    "\n",
    "print(f\"{'Iteration':<12}{'Pixels':<10}{'Train Samples':<15}{'Layers':<10}{'Ancilla Qubits':<18}{'Train Acc':<12}{'Test Acc':<10}{'Timestamp':<20}\")\n",
    "\n",
    "iter_no = 1\n",
    "\n",
    "for number_ancilla in number_ancilla_arr:\n",
    "    for N in N_arr:\n",
    "        for number_train_samples in number_train_samples_arr:\n",
    "            for number_layers in number_layers_arr:\n",
    "                '''\n",
    "                N = 8\n",
    "                number_train_samples = 100 #number of train images per class\n",
    "                number_layers = 6\n",
    "                number_ancilla = 1\n",
    "                '''\n",
    "                new_size = (N,N)\n",
    "                number_test_samples = 5000\n",
    "                # Define the base data directory\n",
    "                data_dir = r\"E:\\Technical\\SchrodingerAI\\data\\RawDataSets\\MNIST\"\n",
    "\n",
    "\n",
    "                input_vectors, target_states_combined, input_state_test_vectors, labelss, encode_length = load_data(N, new_size, number_train_samples, data_dir)\n",
    "\n",
    "                model = QiNN(input_dim=N**2, num_layers=number_layers, num_ancilla = number_ancilla)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "                #loss_fn = nn.CrossEntropyLoss()\n",
    "                loss_fn = nn.MSELoss()\n",
    "\n",
    "                num_epochs = 50\n",
    "                batch_size = number_train_samples * 10\n",
    "\n",
    "                x = input_vectors[0:batch_size, :]\n",
    "                target_labels = target_states_combined[0:batch_size, :]\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    output_joint_state, _ = model(x)\n",
    "\n",
    "                    # Normalize\n",
    "                    output_joint_state = output_joint_state / output_joint_state.norm(dim=1, keepdim=True)\n",
    "                    target_labels = target_labels / target_labels.norm(dim=1, keepdim=True)\n",
    "\n",
    "                    # Convert to probability\n",
    "                    output_probs = torch.abs(output_joint_state) ** 2\n",
    "                    target_probs = torch.abs(target_labels) ** 2\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = loss_fn(output_probs, target_probs)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                #print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "                test_sample_number = 2499\n",
    "\n",
    "                # Select test sample and target\n",
    "                #x_test = input_vectors[test_sample_number, :].unsqueeze(0)\n",
    "                #target_labels_test = target_states_combined[test_sample_number, :].unsqueeze(0)\n",
    "\n",
    "                x_test = input_vectors #[test_sample_number, :].unsqueeze(0)\n",
    "                target_labels_test = target_states_combined #[test_sample_number, :].unsqueeze(0)\n",
    "\n",
    "\n",
    "                # Run through model\n",
    "                output_joint_state_test, _ = model(x_test)\n",
    "\n",
    "\n",
    "                # Normalize\n",
    "                output_joint_state_test = output_joint_state_test / output_joint_state_test.norm(dim=1, keepdim=True)\n",
    "                target_labels_test = target_labels_test / target_labels_test.norm(dim=1, keepdim=True)\n",
    "\n",
    "                # Convert to probability\n",
    "                output_probs_test = torch.abs(output_joint_state_test) ** 2\n",
    "                target_probs_test = torch.abs(target_labels_test) ** 2\n",
    "\n",
    "                predicted_class_test = torch.argmax(output_probs_test, dim=1) // encode_length\n",
    "                true_class_test = torch.argmax(target_probs_test, dim=1) // encode_length\n",
    "\n",
    "                # Compare predicted vs true classes\n",
    "                correct = (predicted_class_test == true_class_test)  # Boolean\n",
    "                num_correct = correct.sum().item()                   # Number of True predictions\n",
    "                total = predicted_class_test.shape[0]                # Total number of predictions\n",
    "\n",
    "                train_accuracy = (num_correct / total) * 100               # Accuracy in percentage\n",
    "\n",
    "                #print(f\"Correct Predictions: {num_correct}/{total}\")\n",
    "                #print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "                # Select test sample and target\n",
    "                #x_test = input_vectors[test_sample_number, :].unsqueeze(0) \n",
    "                #target_labels_test = target_states_combined[test_sample_number, :].unsqueeze(0)  \n",
    "                x_test = input_state_test_vectors #[test_sample_number, :].unsqueeze(0)  \n",
    "\n",
    "                # Run model\n",
    "                output_joint_state_test, _ = model(x_test)\n",
    "\n",
    "                # Normalize\n",
    "                output_joint_state_test = output_joint_state_test / output_joint_state_test.norm(dim=1, keepdim=True)\n",
    "\n",
    "                # Convert to probability\n",
    "                output_probs_test = torch.abs(output_joint_state_test) ** 2\n",
    "\n",
    "                predicted_class_test = torch.argmax(output_probs_test, dim=1) // encode_length\n",
    "                true_class_test = torch.tensor(labelss, dtype = torch.complex128)\n",
    "\n",
    "                # Compare predicted vs true classes\n",
    "                correct = (predicted_class_test == true_class_test)  # Boolean \n",
    "                num_correct = correct.sum().item()                   # Number of True  predictions\n",
    "                total = predicted_class_test.shape[0]                # Total number of predictions\n",
    "\n",
    "                test_accuracy = (num_correct / total) * 100               # Accuracy in percentage\n",
    "\n",
    "                #print(f\"Correct Predictions: {num_correct}/{total}\")\n",
    "                #print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "                timestamp_now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                print(f\"{iter_no:<12}{N:<10}{number_train_samples:<15}{number_layers:<10}{number_ancilla:<18}{train_accuracy:<12.4f}{test_accuracy:<10.4f}{timestamp_now}\")\n",
    "\n",
    "                #print([str(N), str(number_train_samples), str(number_layers), str(number_ancilla),  str(train_accuracy), str(test_accuracy)])\n",
    "                with open(output_file, mode='a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow([\n",
    "                        str(iter_no),\n",
    "                        str(N),\n",
    "                        str(number_train_samples),\n",
    "                        str(number_layers),\n",
    "                        str(number_ancilla),\n",
    "                        f\"{train_accuracy:.4f}\",\n",
    "                        f\"{test_accuracy:.4f}\",\n",
    "                        timestamp_now\n",
    "                    ])\n",
    "                                                                    \n",
    "                iter_no += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
